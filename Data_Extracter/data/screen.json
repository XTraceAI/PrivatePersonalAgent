[{"time": "2025-02-02 09:48:42", "text": "Prize Tracks: Coinbase Developer Platform: Grand Prize: Most Innovative Use of AgentKit ($5k, $3k, $2k) Push onchain boundaries with MPC Wallets & real-world utility e Bonus for reusable building blocks (e.g., Replit Templates, demo repos, etc.) Best Developer Feedback on CDP ($500) e Improve docs/tooling via feedback, PRs, or how-to guides e Must submit a project + fill out our feedback form Best Integration of Other CDP Tools ($500) Seamlessly combine at least two CDP products e Aim for frictionless onchain experiences Viral Consumer App via CDP ($1000) e Dazzle the public with an Al/crypto-powered app primed for virality Everyday tasks, natural-language UX, or autonomous services Infrastructure Prize ($1000) e Build composable layers or fix key agent-system constraints e Multi-agent architectures, cross-network solutions, etc. AgentKit Repo Contribution Award $50/accepted action (max $500/team), $500/framework (max $1000/team) Total budget $7.5k Desired actions/frameworks: MPC, bridging, NFT deployments, yield farming, etc."}, {"time": "2025-02-02 09:48:43", "text": "Felix Hovsepian, PhD - 2nd + Follow Mathematician : Complexity Science 2a. | listen to non-mathematicians speak of #ai - and some even mention the forthcoming era of the machines so intelligent that they will take over as the superior beings on this planet ... they claim these machines will be #agi or #asi And yet, very few appear to have a sound grasp of the #ideas or the #concepts that underpin some of the more sophisticated forms of computation that exists today, in fact, the kind of #thinking that existed when | was a student. Many like the current trend of #genAl because they often tend to be programmers who feel comfortable with sequential ideas, sequential ways of solving problems ... which also appeals to those who from a background in #languages, #history, etc. disciplines which are by their very nature, sequential. Today, | would like to share an idea called #concurrency and the change in #thinking required to build concurrent systems (link in the comments section) #thinkDifferently Some Thoughts About Concurrency by Ivan Sutherland, Visiting Scientist at Portland State University Gy USENIX Subscribe 70 Share eee ust\" 37.3K subscribers ia) | P -~ 6,025 views Jul 23,2010 Our industry has grown up with a sequential model of computing, evolved to husband the logic associated with a few vacuum tubes. Now we must struggle to harness the vast concurrency of modern transistor circuits. Is concurrency fundamentally hard, or does it just seem hard because of our history of sequential programming? | believe some of each. Concurrency is fundamentally hard for only two reasons. One is that concurrent action requires coordination. The other is that concurrent action of many processes can produce an exponential explosion of states. How can we be sure that all such states are benign? Concurrency is easy when we escape its details. Maybe instead of programming sequential processes\" we might better \"configure concurrent communication.\" A communication view of computing matches well the cost structure of modern hardware, where logic is now essentially free but moving data is relatively slow and expensive in time and energy. Making communication central to computation also prepares us for the increasing role geometry will play in the future of computing. New thinking may be essential to harnessing the vast concurrency provided by modern"}, {"time": "2025-02-02 09:48:44", "text": "5. Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeck-R1 ig mor powerful, leveraging cold-start data alongside iterative RL fine-tuning, Ultimately, De pSeek-Ri achieves perfotmance comparable'to OpenAFol-1217 ona range of tasks. We furthetexplore distillation the reasoning capability to-small dense:models. We tise DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense:models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28:9% on AIME and 83:9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction: tuned models based on the'same underlying checkpoints: Tn the future,.we plan to irivest in research across the following directions for DeepSeek:R1. * General Capability: Currently, the capabilities of DeepSeek-R1 fall shortof DeepSeck-V3 in tasks such as function calling, multi-turh, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. * Language Mixing: DeepSeck-R1 is curently optimized fot Chinese and English, whicti may tesult in language mixing issues wh n handling queries in other languages. For instance, DeepSeek-R1 might-use English for reasoning and responses, even if the query is ina language other than English or Chinese: We.aim to-address this limitation in future updates. * Pronipting Engineering: When evaliatinig DeepS ck-R1, we'obsetve thatit is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results: * Software Engineeting Tasks? Due'to the long evaluation times, which impact the ffi- ciency ofthe. RL process, large-scale. RL has not been applied extensively in software etigineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address thi by implementing rejection sampling on software engineering data or incorporating asynchronous valuations dutirig the RL process to intprove efficiency."}, {"time": "2025-02-02 09:48:45", "text": "1. Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It-has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAl s o1 (OpenA], 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of- Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these.methods has achieved general reasoning performance comparable to OpenAl's o1 series models. Ih this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and.a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1- Zero, Upon nearing convergence in the RL process, w create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data-from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, arid then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAl-o1-1217. We further explore distillation from DeepSeek-R1 to.smaller dense models. Using Qwen2.5- 32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru- cial for improving reasoning capabilities. We open-source the distilled Qwen.and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin,.and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models."}, {"time": "2025-02-02 09:48:46", "text": "About this item [Compatible with 2018-2024 iPad & iPad Pro] The stylus supports for iPad series 2018-2024 model: iPad Pro 13\" (M4), iPad Pro 12.9\" (3rd/4th/5th/6th), iPad Pro 11\"(All Generations), iPad Air M2/5th/4th/3rd Gen, iPad 10th/9th/8th/7th/6th, iPad Mini 6th/5th Gen. Please confirm your devices before you place an order, another model is invalid [Not compatible with models before 2018] The stylus doesn't support iPad pro 1st&2nd, iPad pro 10.5\", iPad pro 9.7\", iPad 1 to 5 Gen, iPad mini 1 to 4 Gen, iPad Air 1st/2nd Gen versions of iPad. Not Compatible With iPhone, Android, Microsoft devices.iPad 10gen cannt use magnetic attraction e [Precise and Smooth] 1.5mm pen tip can replace your finger to execute finer instructions, it easy to install and tear off the tips on your stylus pen without any tool. No lag/offset/breaking point ! Compared with the ordinary stylus pen, it has higher sensitivity, more accurate signal, more comfortable hand. Not easy to break! [ Note] The stylus pen no pressure-sensitive design [Palm Rejection Design] Stylus pen with palm rejection technology provides a natural writing feeling and quick, effortless interaction with your screen, gives you more accuracy and control against the screen. We commend you to use this pen on the iPad with a glass screen protector [Touch Switch & Fast Charging] No need for Bluetooth connection, turn on this stylus pen by simply touching the cap button,charging 15- 20 minutes, can support the work of 8-10 hours [Note] This stylus pen does not come with a pen cap or a charging port cover. When charging the stylus pen, please use a 5V2A power adapter to charge the product."}, {"time": "2025-02-02 09:48:47", "text": "Deals for the weekend LE ee STEELE a cs \\ oe a Se ee\" Ls ate 6 2 eee i ch Pa] aaa HL a) (Es a + in e 4 r - . 5 Be) an, oN, \\| uJ Se | mit ars i) ne FONTAINEBLEAU - ee: Vie SN _ 4 tow uy i _ imi = I ~ % is _. a ak eau! | Genius | | Genius | Fontainebleau Las Vegas | Genius | Hotel Aventura Courtyard by Marriott San Las Vegas, United States of America The LINE Hotel LA Los Angeles, United States of America Diego Mission Valley/Hotel... 8.9] Excellent - 1.527 reviews Los Angeles, United States of America Very Good - 1,607 reviews San Diego, United States of America Good - 1,840 reviews 8 | Very Good - 3,649 reviews 2 nights $804 $260 2 nights $348 $282 2nights $766 $608 2 nights $578 $414"}, {"time": "2025-02-02 09:48:48", "text": "1.1. Contributions Post-Trainting: Large-Scale Reinforcement Learning on the Base Model We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to.explore chain-of thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek- R1-Zero. demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the. first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements.in this area. We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human pref- erences,-as well as two SFT stages that serve as the seed for the model s reasoning.and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. Distillation: Smaller Models Can Be Powerful Too . We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models: The open source DeepSeek-R1,-as well as its API, will benefit the research community to distill better smaller.models in the future. * Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well.on benchmarks. DeepSeek- R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi- tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6%.on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These tesults significantly outperform previous. open- source models and are compatable to.ol-mini. We open-source distilled 1.5B,.7B, 8B, 14B, 328, anid 70B checkpoints based on Qwen2.5 and Llama3 series to the community: 1.2. Summary-of Evaluation Results Reasoning tasks: (i) DeepSeek-R1 achieves a'score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of: 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, a8 it achieves 2,029 Elo rating on Cod forces outperforming.96.3% human participants in. the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. * Knowledge: On benchmarks such as MMLU; MMLU-Pi0, and:-GPQA Diamond, DeepSeek- Ri achieves outstanding results, significantly outperforming DeepSeek-V3 with scores. of 90.8% on MMLU; 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its: performances slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-ol surpasses 40 on this benchmark."}, {"time": "2025-02-02 09:48:49", "text": "a Google congtic 1600 Amphitheatre Pkwy 1 Mountain View, CA 94043 | n VO | ce United States Invoice number: 5164497546 Federal Tax ID: 77-0493581 Bill to Felix Meng XTrace 399 Fremont Street Unit 2403 San Francisco, CA 94105 United States Details Google Workspace Invoice date ............. eeeeeeeeeeeee- dan 31, 2025 Total in USD $72.00 Billing ID ............2.00.. eee. 7280-0454-7310 Domain name oes seeeeeseeeess trace al Summary for Jan 1, 2025 - Jan 31, 2025 Subtotal in USD $72.00 Tax (0%) $0.00 Total in USD $72.00 You will be automatically charged for any amount due."}]