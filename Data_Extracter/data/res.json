[
    {
        "time": "2025-01-31 18:22:06",
        "text": " Dario Amade, former employee at OpenAI and current CEO and founder of Anthropik has dropped a new essay in which he details his thoughts on DeepSeq model innovation and GPU export controls to China. This combined with new evidence that shows DeepSeq may have actually distilled a lot of the data from OpenAI's models is causing a stir in the AI industry. Let me break it all down. So here's his new essay and it is incredible. Now before we get to this, I want to show you a few of the rumors and possible evidence that DeepSeq actually stole data from OpenAI to create the R1 model. So first in this Axios article, OpenAI says DeepSeq may have inappropriately used its models output. Now may have is not a strong statement. They're not saying definitely did. But at the same time, there was evidence in the fact that the DeepSeq R1 model itself says it was trained by OpenAI. Here's an example posted to the OpenAI subreddit right here. I was developed by OpenAI, a company founded in December 2015 by Sam Altman, Elon Musk, Greg Brockman, Ilya Suskover and so on. And so I've seen a few of these instances in which DeepSeq just says it was trained by OpenAI. Now that doesn't necessarily make it true. Obviously models hallucinate all the time and there could be a number of reasons that DeepSeq R1 has said this, but it is interesting. Let me show you a couple of the things. Now this is Jonathan Ross. CEO and founder of GROC. He specifically talks about how distillation works. And if you're not familiar with distillation, it is basically a small model learning from a big model. That is the most simplistic way to put it. Let's hear how Jonathan Ross explains what might have happened. If there's a really good model already right here, just have it generate the data and you go right up to where it is. And that's what they did. So it is true that they spent about six million or whatever it was on the training. They spent a lot more distilling or scraping the OpenAI model. So a lot of the top people in the industry, and this is not just anthropic leaders, OpenAI leaders, this is GROC's leader. And GROC is an inference provider and they only work with open source models right now. So he has not incentivized in any way to say these things as far as I can tell. So what he's basically saying and what a lot of people are kind of triangulating around is that, first of all, the five million dollar number to train R1, maybe, but really they spent a ton more before that. And I'm going to get to that. Apparently they have 50,000 GPUs, but they spent a lot more on that in the research and development and they also spent a lot more than that in the actual scraping of OpenAI's models. And so what a lot of people are saying is it is distilled down from an OpenAI model. Now that isn't taking away some of the innovations that the DeepSeek team has made and the DeepSeek team seems to be legit. They seem to be very smart and genuine with their efforts to push frontier AI. But I just want to put all of this in context for you and report on the new findings that I'm seeing around the web. Now here's David Sacks, AI and CryptoZar for the United States, essentially saying the same thing. Let's watch. There's a technique in AI called distillation, which you're going to hear a lot about. And it's when one model learns from another model. Effectively, what happens is that the student model asked the parent model a lot of questions, just like a human would learn. But AIs can do this, asking millions of questions and they can essentially mimic the reasoning process that they learned from the parent model and they can kind of suck the knowledge out of the parent model. And there's substantial evidence that what DeepSeek did here is they distilled the knowledge out of OpenAI's models. And I don't think OpenAI is very happy about this. All right. So he is saying the same thing. Now let's get to the main reason we're here today, Dario's essay. So Dario says, my thoughts on China export controls and two possible futures. And you could probably start to guess what two futures he's referencing are. Thanks to the sponsor of this segment, Recraft. Recraft is an image generation and editing tool. You've probably heard of it if you've watched this channel at all. V3 model secures the number one spot on the hugging face leaderboards, surpassing mid journey and OpenAI's Dolly with an impressive 1152 E-Lo rating. This is huge because Hugging Face is the leading platform for evaluating text to image models. So it has some stiff competition, but it was still number one. So why is Recraft so powerful? Well, it thinks in design language, giving you full control over your visuals, which is not something other models can say. Photo realistic images, posters, complex scenes, Recraft can do it all in a stunning way. And for realistic images, they are truly all of them. Here's a QR code expected and the reaction of the markets dropping in video by 16% was a stunning overreaction. And by the way, I agree with that. That is insane that that happened and really shows that a lot of analysts and investors truly do not understand what's going on in artificial intelligence. So he starts with, I won't focus on whether deep seek is or isn't a threat to USAI companies like Anthropic, although I do believe many of the claims about their threat to USAI leadership are greatly overstated. Okay, so right out of the gate saying this whole thing was blown way out of proportion, but he does say export policies become even more existentially important than they were a week ago. Let me break that down for you. What do export controls actually do? What do they mean? Well, right now there are export controls, which make it illegal to export cutting edge GPUs and AI chips to China. Now the reason for that is we don't want China building out AI infrastructure that would eventually allow them to maybe reach AGI and ASI before that because at that point it's too late. Then we live in a unipolar world. China is the leader and really there's no catching up after that. And so these policies are put in place so China cannot get their hands on vast amounts of cutting edge GPUs, generally from NVIDIA, like the H100. But these export controls don't always work. At least there's some evidence that DeepSeek and China in general has some H100s and they do that through Shell companies. They do that through smuggling kind of crazy stuff to be honest. I'd love to cover that in more detail because it's interesting from a geopolitical standpoint, it's interesting from a technology standpoint and yeah, it just sounds like a movie. Now he goes on to talk about the three dynamics of artificial intelligence. If you've watched his channel at all, you've heard me talk about this quite a bit. Now remember, Dario worked at OpenAI for a while before starting Anthropic. So Dario and his Anthropic co-founders were among the first to document the scaling laws back when he was working at OpenAI. All else equal scaling up the training of AI systems leads to smoothly better results on a range of cognitive tasks across the board. Basically every order of magnitude increase in the scale of a training run or the amount of compute you're throwing at something, the smooth linear increase in quality you're going to see. So he gives some numbers. A million dollar model might solve 20% of important coding tasks. 10 million, so that's 10x, might solve 40%. A hundred million, that's a hundred x, might solve 60%. So exponential increase in spend, linear increase in quality. That is one of the scaling laws. These differences tend to have huge implications in practice. Another factor of 10 may correspond to the difference between an undergraduate and PhD skill level and thus companies are investing heavily in training these models. And that's the thing, they get exponentially more expensive to get these linear improvements. And so it really does take a crazy amount of money, a crazy amount of compute to actually squeeze out these improvements in quality. Next is the shifting curve. And here he talks about. Hi, I'm premium wireless for $15 a month at Mint Mobile. What I'm premium wireless for $15.00. About both on the software and architecture side and the hardware side improvements that provide huge jumps in efficiency and quality. So for example, it could be an improvement to the architecture of the model. A tweak to the basic transformer architecture. New generations of hardware also have the same effect. And so what you're seeing is these improvements in efficiency, these innovations could result in a 1.2 x improvement, a 2 x, a 4 x improvement. And so that is the shifting curve that he's referring to every frontier AI company regularly discovers many of these CMs. That's compute multipliers. Only small ones, 1.2 x, sometimes medium ones, 2 x and every once in a while, very large ones 10 x. I want to read this slowly because this is critically important. Listen to this. Because the value of having a more intelligent system is so high, this shifting of the curve typically causes companies to spend more, not less on training models. The gains in cost efficiency end up entirely devoted to training smarter models, limited only by the company's financial resources. So that is a mixture of Jevons paradox and just the nature of intelligence exploding right before our eyes. It's not that we're going to reach some plateau one day and companies are going to say, okay, we're done. No more training, no more inference needed. No, no, it's quite the opposite. Every time we have this huge unlock in terms of efficiency or cost reduction, all of the compute available continues to be used in full. It just goes towards whatever that next step up is. People are naturally attracted to the idea that first something is expensive, then it gets"
    },
    {
        "time": "2025-02-02 09:48:42",
        "text": "Prize Tracks: Coinbase Developer Platform: Grand Prize: Most Innovative Use of AgentKit ($5k, $3k, $2k) Push onchain boundaries with MPC Wallets & real-world utility e Bonus for reusable building blocks (e.g., Replit Templates, demo repos, etc.) Best Developer Feedback on CDP ($500) e Improve docs/tooling via feedback, PRs, or how-to guides e Must submit a project + fill out our feedback form Best Integration of Other CDP Tools ($500) Seamlessly combine at least two CDP products e Aim for frictionless onchain experiences Viral Consumer App via CDP ($1000) e Dazzle the public with an Al/crypto-powered app primed for virality Everyday tasks, natural-language UX, or autonomous services Infrastructure Prize ($1000) e Build composable layers or fix key agent-system constraints e Multi-agent architectures, cross-network solutions, etc. AgentKit Repo Contribution Award $50/accepted action (max $500/team), $500/framework (max $1000/team) Total budget $7.5k Desired actions/frameworks: MPC, bridging, NFT deployments, yield farming, etc."
    },
    {
        "time": "2025-02-02 09:48:43",
        "text": "Felix Hovsepian, PhD - 2nd + Follow Mathematician : Complexity Science 2a. | listen to non-mathematicians speak of #ai - and some even mention the forthcoming era of the machines so intelligent that they will take over as the superior beings on this planet ... they claim these machines will be #agi or #asi And yet, very few appear to have a sound grasp of the #ideas or the #concepts that underpin some of the more sophisticated forms of computation that exists today, in fact, the kind of #thinking that existed when | was a student. Many like the current trend of #genAl because they often tend to be programmers who feel comfortable with sequential ideas, sequential ways of solving problems ... which also appeals to those who from a background in #languages, #history, etc. disciplines which are by their very nature, sequential. Today, | would like to share an idea called #concurrency and the change in #thinking required to build concurrent systems (link in the comments section) #thinkDifferently Some Thoughts About Concurrency by Ivan Sutherland, Visiting Scientist at Portland State University Gy USENIX Subscribe 70 Share eee ust\" 37.3K subscribers ia) | P -~ 6,025 views Jul 23,2010 Our industry has grown up with a sequential model of computing, evolved to husband the logic associated with a few vacuum tubes. Now we must struggle to harness the vast concurrency of modern transistor circuits. Is concurrency fundamentally hard, or does it just seem hard because of our history of sequential programming? | believe some of each. Concurrency is fundamentally hard for only two reasons. One is that concurrent action requires coordination. The other is that concurrent action of many processes can produce an exponential explosion of states. How can we be sure that all such states are benign? Concurrency is easy when we escape its details. Maybe instead of programming sequential processes\" we might better \"configure concurrent communication.\" A communication view of computing matches well the cost structure of modern hardware, where logic is now essentially free but moving data is relatively slow and expensive in time and energy. Making communication central to computation also prepares us for the increasing role geometry will play in the future of computing. New thinking may be essential to harnessing the vast concurrency provided by modern"
    },
    {
        "time": "2025-02-02 09:48:44",
        "text": "5. Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeck-R1 ig mor powerful, leveraging cold-start data alongside iterative RL fine-tuning, Ultimately, De pSeek-Ri achieves perfotmance comparable'to OpenAFol-1217 ona range of tasks. We furthetexplore distillation the reasoning capability to-small dense:models. We tise DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense:models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28:9% on AIME and 83:9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction: tuned models based on the'same underlying checkpoints: Tn the future,.we plan to irivest in research across the following directions for DeepSeek:R1. * General Capability: Currently, the capabilities of DeepSeek-R1 fall shortof DeepSeck-V3 in tasks such as function calling, multi-turh, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. * Language Mixing: DeepSeck-R1 is curently optimized fot Chinese and English, whicti may tesult in language mixing issues wh n handling queries in other languages. For instance, DeepSeek-R1 might-use English for reasoning and responses, even if the query is ina language other than English or Chinese: We.aim to-address this limitation in future updates. * Pronipting Engineering: When evaliatinig DeepS ck-R1, we'obsetve thatit is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results: * Software Engineeting Tasks? Due'to the long evaluation times, which impact the ffi- ciency ofthe. RL process, large-scale. RL has not been applied extensively in software etigineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address thi by implementing rejection sampling on software engineering data or incorporating asynchronous valuations dutirig the RL process to intprove efficiency."
    },
    {
        "time": "2025-02-02 09:48:45",
        "text": "1. Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It-has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAl s o1 (OpenA], 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of- Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these.methods has achieved general reasoning performance comparable to OpenAl's o1 series models. Ih this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and.a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1- Zero, Upon nearing convergence in the RL process, w create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data-from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, arid then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAl-o1-1217. We further explore distillation from DeepSeek-R1 to.smaller dense models. Using Qwen2.5- 32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru- cial for improving reasoning capabilities. We open-source the distilled Qwen.and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin,.and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models."
    },
    {
        "time": "2025-02-02 09:48:46",
        "text": "About this item [Compatible with 2018-2024 iPad & iPad Pro] The stylus supports for iPad series 2018-2024 model: iPad Pro 13\" (M4), iPad Pro 12.9\" (3rd/4th/5th/6th), iPad Pro 11\"(All Generations), iPad Air M2/5th/4th/3rd Gen, iPad 10th/9th/8th/7th/6th, iPad Mini 6th/5th Gen. Please confirm your devices before you place an order, another model is invalid [Not compatible with models before 2018] The stylus doesn't support iPad pro 1st&2nd, iPad pro 10.5\", iPad pro 9.7\", iPad 1 to 5 Gen, iPad mini 1 to 4 Gen, iPad Air 1st/2nd Gen versions of iPad. Not Compatible With iPhone, Android, Microsoft devices.iPad 10gen cannt use magnetic attraction e [Precise and Smooth] 1.5mm pen tip can replace your finger to execute finer instructions, it easy to install and tear off the tips on your stylus pen without any tool. No lag/offset/breaking point ! Compared with the ordinary stylus pen, it has higher sensitivity, more accurate signal, more comfortable hand. Not easy to break! [ Note] The stylus pen no pressure-sensitive design [Palm Rejection Design] Stylus pen with palm rejection technology provides a natural writing feeling and quick, effortless interaction with your screen, gives you more accuracy and control against the screen. We commend you to use this pen on the iPad with a glass screen protector [Touch Switch & Fast Charging] No need for Bluetooth connection, turn on this stylus pen by simply touching the cap button,charging 15- 20 minutes, can support the work of 8-10 hours [Note] This stylus pen does not come with a pen cap or a charging port cover. When charging the stylus pen, please use a 5V2A power adapter to charge the product."
    },
    {
        "time": "2025-02-02 09:48:47",
        "text": "Deals for the weekend LE ee STEELE a cs \\ oe a Se ee\" Ls ate 6 2 eee i ch Pa] aaa HL a) (Es a + in e 4 r - . 5 Be) an, oN, \\| uJ Se | mit ars i) ne FONTAINEBLEAU - ee: Vie SN _ 4 tow uy i _ imi = I ~ % is _. a ak eau! | Genius | | Genius | Fontainebleau Las Vegas | Genius | Hotel Aventura Courtyard by Marriott San Las Vegas, United States of America The LINE Hotel LA Los Angeles, United States of America Diego Mission Valley/Hotel... 8.9] Excellent - 1.527 reviews Los Angeles, United States of America Very Good - 1,607 reviews San Diego, United States of America Good - 1,840 reviews 8 | Very Good - 3,649 reviews 2 nights $804 $260 2 nights $348 $282 2nights $766 $608 2 nights $578 $414"
    },
    {
        "time": "2025-02-02 09:48:48",
        "text": "1.1. Contributions Post-Trainting: Large-Scale Reinforcement Learning on the Base Model We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to.explore chain-of thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek- R1-Zero. demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the. first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements.in this area. We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human pref- erences,-as well as two SFT stages that serve as the seed for the model s reasoning.and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. Distillation: Smaller Models Can Be Powerful Too . We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models: The open source DeepSeek-R1,-as well as its API, will benefit the research community to distill better smaller.models in the future. * Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well.on benchmarks. DeepSeek- R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi- tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6%.on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These tesults significantly outperform previous. open- source models and are compatable to.ol-mini. We open-source distilled 1.5B,.7B, 8B, 14B, 328, anid 70B checkpoints based on Qwen2.5 and Llama3 series to the community: 1.2. Summary-of Evaluation Results Reasoning tasks: (i) DeepSeek-R1 achieves a'score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of: 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, a8 it achieves 2,029 Elo rating on Cod forces outperforming.96.3% human participants in. the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. * Knowledge: On benchmarks such as MMLU; MMLU-Pi0, and:-GPQA Diamond, DeepSeek- Ri achieves outstanding results, significantly outperforming DeepSeek-V3 with scores. of 90.8% on MMLU; 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its: performances slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-ol surpasses 40 on this benchmark."
    },
    {
        "time": "2025-02-02 09:48:49",
        "text": "a Google congtic 1600 Amphitheatre Pkwy 1 Mountain View, CA 94043 | n VO | ce United States Invoice number: 5164497546 Federal Tax ID: 77-0493581 Bill to Felix Meng XTrace 399 Fremont Street Unit 2403 San Francisco, CA 94105 United States Details Google Workspace Invoice date ............. eeeeeeeeeeeee- dan 31, 2025 Total in USD $72.00 Billing ID ............2.00.. eee. 7280-0454-7310 Domain name oes seeeeeseeeess trace al Summary for Jan 1, 2025 - Jan 31, 2025 Subtotal in USD $72.00 Tax (0%) $0.00 Total in USD $72.00 You will be automatically charged for any amount due."
    }
]